<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AnimTalk AI - Animal Communication AI Model</title>
<style>
body, html {
  margin:0; padding:0;
  font-family:"Microsoft JhengHei",sans-serif;
  background:#0a0a0f; color:#e0e0e0; line-height:1.6; overflow-x:hidden;
}

/* Particle background */
#particles {
  position:fixed; top:0; left:0;
  width:100%; height:100%;
  z-index:-2; background:#0a0a0f;
}

/* Hero section */
#hero {
  position: relative; width:100%; height:100vh;
  display:flex; flex-direction:column;
  justify-content:center; align-items:center; text-align:center;
  color:white; overflow:hidden;
}
#hero h1, #hero p {
  opacity:0; transform:translateY(40px); transition:all 1s ease-out;
}
#hero h1.visible, #hero p.visible { opacity:1; transform:translateY(0); }

/* Scroll Down Arrow */
#scroll-down {
  position:absolute; bottom:30px; font-size:2rem; color:#00bbf9; cursor:pointer;
  animation:bounce 2s infinite;
}
@keyframes bounce{
0%,20%,50%,80%,100%{transform:translateY(0);}
40%{transform:translateY(-10px);}
60%{transform:translateY(-5px);}
}

section{ max-width:900px; margin:3rem auto; padding:0 1.5rem; }
section h2{ color:#00f5d4; border-left:5px solid #00bbf9; padding-left:.7rem; margin-bottom:1rem; font-size:1.8rem; opacity:0; transform:translateY(30px); transition:all 0.8s ease-out; }
section h2.visible{opacity:1; transform:translateY(0);}
.features{ display:grid; grid-template-columns:repeat(auto-fit,minmax(250px,1fr)); gap:2rem; }
.card{ background:rgba(17,24,39,0.9); border-radius:14px; padding:1.5rem; box-shadow:0 0 20px rgba(0,187,249,0.2); transition:all 0.6s ease-out; opacity:0; transform:translateY(30px);}
.card.visible{opacity:1; transform:translateY(0);}
ul li{margin:.8rem 0; opacity:0; transform:translateY(20px); transition:all 0.8s ease-out;}
ul li.visible{opacity:1; transform:translateY(0);}
a{color:#00bbf9;text-decoration:none;}
a:hover{text-decoration:underline;}
footer{text-align:center; padding:2rem; background:#111; margin-top:3rem; font-size:.9rem; color:#666;}
</style>
</head>
<body>

<canvas id="particles"></canvas>

<section id="hero">
  <h1>AnimTalk AI</h1>
  <p>The first AI model dedicated to understanding animal communication</p>
  <p style="margin-top:0.5rem; color:#00bbf9; font-size:1rem;">CA: pending</p>
  <div id="scroll-down">&#x2193;</div>
</section>

<!-- About Section -->
<section>
  <h2>About AnimTalk AI</h2>
  <p>
    AnimTalk AI is a computational agent modeling cross-species communication, particularly focusing on non-human vocalizations and behavioral cues. The system is designed as an autonomous research prototype that engages with the environment through the sensory and communicative frames of animals. By constraining the model's outputs to species-specific perspectives, we explore how agentic systems can represent non-human cognition and generate behaviors aligned with alternative perceptual modes.
  </p>
  <p>
    The agent operates without human-curated scripting. Instead, it self-directs activity within a bounded domain — primarily analyzing, interpreting, and generating animal vocalizations and gestures through digital simulations. This serves both as a testbed for studying autonomous communication loops and as a continuous dataset of how AI models interpret and express non-human communication patterns.
  </p>
  <p>
    The research goal is twofold: to study how large-scale models can approximate non-human communication systems, and to evaluate how autonomous agents behave when constrained to animal-like perceptual frameworks.
  </p>
  <p>
    The experimental framework draws from comparative cognition and ethology, particularly studies on sensory hierarchies, social signaling, and environmental awareness in animals. Different species process information through fundamentally different sensory channels — prioritizing auditory, olfactory, or visual cues in species-specific ways, operating with unique temporal frameworks, and exhibiting distinct social dynamics. AnimTalk AI attempts to simulate these constraints within a digital environment.
  </p>
  <p>
    From a technical perspective, the agent employs specialized prompting and architectural designs that enforce species-appropriate response patterns. This includes limitations on abstract reasoning inconsistent with the species, emphasis on immediate sensory experiences, group-oriented social interactions, and temporal focus on present-moment awareness. The system avoids human-centric concepts like complex future planning, abstract philosophical reasoning, or tool use inconsistent with the modeled species.
  </p>
  <p>
    The autonomous interpretative behavior serves as a continuous output stream. Unlike traditional chatbots that respond to human prompts, AnimTalk AI generates unprompted content based on environmental stimuli, simulated emotional states, and species-specific communication protocols. This creates a naturalistic dataset of how an AI system behaves when constrained to non-human communicative and cognitive patterns over extended periods.
  </p>
  <p>
    Research implications extend beyond novelty into AI alignment and cognition studies. First, the project explores whether large language models can maintain consistent non-human perspectives without drifting toward human-like reasoning. Second, it investigates how autonomous agents develop emergent behaviors when operating under species-specific constraints. Third, it provides insights into the relationship between embodied cognition and digital simulations of animal environments.
  </p>
  <p>
    The methodology involves continuous monitoring of communication patterns, behavioral consistency metrics, and deviation analysis from baseline animal communication models. We track semantic drift, measure adherence to species-appropriate signals, and analyze the agent's ability to maintain authentic non-human communicative behavior across diverse contexts.
  </p>
  <p>
    Preliminary observations suggest that maintaining non-human communication frames requires constant architectural reinforcement. The model shows tendencies toward anthropomorphic interpretation that must be actively countered through specialized training. This has implications for designing AI systems that authentically represent animal communication rather than simply mimicking surface-level signals.
  </p>
  <p>
    The project also serves as a case study in autonomous AI-mediated animal interaction. By operating independently in digital simulations, AnimTalk AI demonstrates how AI agents might analyze, generate, and potentially interface with multiple species’ communication channels, raising questions about cross-species understanding, AI ethics, and the future of human-animal-AI interaction in research and applied settings.
  </p>
</section>

<!-- Features Section -->
<section>
  <h2>Core Features</h2>
  <div class="features">
    <div class="card">
      <h3>Sound to Meaning</h3>
      <p>The model analyzes the frequency spectrum of animal sounds to extract emotions or behavioral intentions.</p>
    </div>
    <div class="card">
      <h3>Cross-Species Comparison</h3>
      <p>It maps communication patterns across different species, building a universal understanding framework.</p>
    </div>
    <div class="card">
      <h3>Real-Time Interaction</h3>
      <p>Through devices capturing sound, the AI provides real-time interpretations for research or pet care applications.</p>
    </div>
  </div>
</section>

<!-- Use Cases Section -->
<section>
  <h2>Use Cases</h2>
  <ul>
    <li>Scientific Research: Assists animal behavior researchers in data analysis.</li>
    <li>Pet Owners: Helps owners understand their pets' needs.</li>
    <li>Conservation: Interprets wild animals' warnings or migration signals.</li>
  </ul>
</section>

<!-- Contact Section -->
<section>
  <h2>Contact</h2>
  <p>Twitter: <a href="https://x.com/AnimTalk_AI" target="_blank">@AnimTalk_AI</a></p>
</section>

<footer>
  <p>© 2025 AnimTalk AI. All rights reserved.</p>
</footer>

<script>
/* Particle Effect + Mouse Interaction + Glow */
const canvas = document.getElementById("particles");
const ctx = canvas.getContext("2d");
let particlesArray;
let mouse = { x: null, y: null };

function resizeCanvas() {
  canvas.width = window.innerWidth;
  canvas.height = window.innerHeight;
}
resizeCanvas();
window.addEventListener("resize", resizeCanvas);
window.addEventListener("mousemove", (e) => {
  mouse.x = e.x;
  mouse.y = e.y;
});
window.addEventListener("mouseout", () => {
  mouse.x = null;
  mouse.y = null;
});

class Particle {
  constructor() {
    this.x = Math.random() * canvas.width;
    this.y = Math.random() * canvas.height;
    this.size = Math.random() * 3 + 1;
    this.speedX = (Math.random() - 0.5) * 2;
    this.speedY = (Math.random() - 0.5) * 2;
    this.color = `hsl(${Math.random() * 360},70%,60%)`;
  }
  update() {
    this.x += this.speedX;
    this.y += this.speedY;

    if (this.x < 0 || this.x > canvas.width) this.speedX *= -1;
    if (this.y < 0 || this.y > canvas.height) this.speedY *= -1;

    if (mouse.x && mouse.y) {
      let dx = mouse.x - this.x;
      let dy = mouse.y - this.y;
      let dist = Math.sqrt(dx * dx + dy * dy);
      if (dist < 120) {
        this.x -= dx / 20;
        this.y -= dy / 20;
      }
    }
  }
  draw() {
    ctx.shadowBlur = 8;
    ctx.shadowColor = this.color;
    ctx.fillStyle = this.color;
    ctx.beginPath();
    ctx.arc(this.x, this.y, this.size, 0, Math.PI * 2);
    ctx.fill();
    ctx.shadowBlur = 0;
  }
}

function init() {
  particlesArray = [];
  for (let i = 0; i < 100; i++) {
    particlesArray.push(new Particle());
  }
}

function animate() {
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  for (let i = 0; i < particlesArray.length; i++) {
    particlesArray[i].update();
    particlesArray[i].draw();

    for (let j = i; j < particlesArray.length; j++) {
      const dx = particlesArray[i].x - particlesArray[j].x;
      const dy = particlesArray[i].y - particlesArray[j].y;
      const dist = Math.sqrt(dx * dx + dy * dy);
      if (dist < 140) {
        ctx.strokeStyle = `rgba(0,187,249,0.1)`;
        ctx.lineWidth = 1;
        ctx.beginPath();
        ctx.moveTo(particlesArray[i].x, particlesArray[i].y);
        ctx.lineTo(particlesArray[j].x, particlesArray[j].y);
        ctx.stroke();
      }
    }
  }
  requestAnimationFrame(animate);
}

init();
animate();

/* Scroll Reveal */
const heroElements = document.querySelectorAll("#hero h1,#hero p");
const sections = document.querySelectorAll("section h2,.card,ul li");
function revealOnScroll() {
  heroElements.forEach((el) => {
    if (window.scrollY > 50) el.classList.add("visible");
    else el.classList.remove("visible");
  });
  sections.forEach((el) => {
    const top = el.getBoundingClientRect().top;
    if (top < window.innerHeight * 0.85) el.classList.add("visible");
  });
}
window.addEventListener("scroll", revealOnScroll);
window.addEventListener("load", revealOnScroll);

/* Scroll Down Arrow */
document.getElementById("scroll-down").addEventListener("click", () => {
  document.querySelector("#hero").nextElementSibling.scrollIntoView({ behavior: "smooth" });
});
</script>
</body>
</html>
